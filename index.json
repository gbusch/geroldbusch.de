[{"content":"","date":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"Since I usually use Azure, I thought \u0026ldquo;let\u0026rsquo;s try out Databricks on AWS for fun\u0026rdquo; and was surprised how many resources were created that I completely lost track of\u0026hellip; In retrospective, terraform would have been a better choice so that I can destroy resources with one command. So: how would I clean up AWS and delete all resources?\nAWS-nuke is a helpful script to destroy all resources within an AWS account. Let\u0026rsquo;s go through all the steps.\nInstall aws-nuke: on a mac simply with brew install aws-nuke (for other OS see the aws-nuke github page). Setup the AWS CLI with brew install awscli and then aws configure. You will then have to enter AWS Access Key ID, AWS Secret Access Key, Default region name, Default output format (e.g. json). To get the keys, go to \u0026ldquo;IAM\u0026rdquo;, \u0026ldquo;Users\u0026rdquo;, choose the relevant user, then \u0026ldquo;Security credential\u0026rdquo;, then down to \u0026ldquo;Access keys\u0026rdquo;. Now configure aws-nuke. Create a file, for example nuke-config.yml. Parameters are The list of regions to nuke, here I tried to select all. An account-blocklist. Here you should list the accounts that should not be nuked. In a usecase, where you have a production account and several development accounts that you want to clean up from time to time, list the production account here. In my case, I just have one development account, so I listed a dummy account here. The account to nuke (find out the account ID by clicking on your account drop-down menu in the upper right corner of the AWS console) and a list of filters. In my case I wanted to keep my admin account and the corresponding key as well as the Billing Alarm. But you can list any resource here. regions: - \u0026#34;global\u0026#34; # This is for all global resource types e.g. IAM - \u0026#34;us-east-2\u0026#34; - \u0026#34;us-east-1\u0026#34; - \u0026#34;us-west-1\u0026#34; - \u0026#34;us-west-2\u0026#34; - \u0026#34;af-south-1\u0026#34; - \u0026#34;ap-east-1\u0026#34; - \u0026#34;ap-south-1\u0026#34; - \u0026#34;ap-northeast-3\u0026#34; - \u0026#34;ap-northeast-2\u0026#34; - \u0026#34;ap-southeast-1\u0026#34; - \u0026#34;ap-southeast-2\u0026#34; - \u0026#34;ap-northeast-1\u0026#34; - \u0026#34;ca-central-1\u0026#34; - \u0026#34;cn-north-1\u0026#34; - \u0026#34;cn-northwest-1\u0026#34; - \u0026#34;eu-central-1\u0026#34; - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; - \u0026#34;eu-south-1\u0026#34; - \u0026#34;eu-west-3\u0026#34; - \u0026#34;eu-north-1\u0026#34; - \u0026#34;me-south-1\u0026#34; - \u0026#34;sa-east-1\u0026#34; account-blocklist: - \u0026#34;999999999999\u0026#34; # production accounts: \u0026#34;6**********2\u0026#34;: filters: IAMUser: - \u0026#34;gbusch\u0026#34; IAMUserPolicyAttachment: - \u0026#34;gbusch -\u0026gt; AdministratorAccess\u0026#34; IAMUserAccessKey: - \u0026#34;gbusch -\u0026gt; A*************H\u0026#34; CloudWatchAlarm: - \u0026#34;BillingAlarm\u0026#34; A dry-run will show you all resources and what aws-nuke is planning to do with it: aws-nuke -c nuke-config.yml --profile default. To actually delete resources, run with the no-dry-run-flag: aws-nuke -c nuke-config.yml --profile default --no-dry-run. ","date":"12 March 2023","permalink":"/blog/clean-up-aws-account/","section":"Blog","summary":"Since I usually use Azure, I thought \u0026ldquo;let\u0026rsquo;s try out Databricks on AWS for fun\u0026rdquo; and was surprised how many resources were created that I completely lost track of\u0026hellip; In retrospective, terraform would have been a better choice so that I can destroy resources with one command.","title":"Clean up your AWS account"},{"content":"","date":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/apache-spark/","section":"Tags","summary":"","title":"Apache Spark"},{"content":"Recently, we had the following challenge:\nData should be retrieved by REST-request in regular time intervals The data (simple json-response) should be ingested into the data lake The files should then be further processed. Our solution was the following:\nWrite a python script to perform the requests. The script should persist the response as separate json-files into the data lake. Schedule this script using Databricks jobs. As a second task of this multi-task job, trigger a Delta Live Tables pipeline. The Delta Live Table pipeline should start using the Autoloader capability. Autoloader keeps track of which files are new within the data lake and only processes new files. Internally this is handled using Event Hubs but you don\u0026rsquo;t need to care for details because this is all hidden from you.\nLet\u0026rsquo;s have a look at an Autoloader example:\nimport dlt from pyspark.sql import functions as F @dlt.table( name=\u0026#34;table_name\u0026#34;, path=\u0026#34;/mnt/datalake/02_silver/table.delta\u0026#34;, partition_cols=[\u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;], ) def ingested_data(): return ( spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .option( \u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;ratingAverage float, ratingsDistribution array\u0026lt;int\u0026gt;, votesTotal int, country string, city string\u0026#34;, ) .option(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;, \u0026#34;rescue\u0026#34;) .load(f\u0026#34;/mnt/datalake/01_bronze/raw-data/\u0026#34;) .withColumn(\u0026#34;file_name\u0026#34;, F.input_file_name()) ) This returns a Spark Streaming object that can then be used like other streaming objects, for example data coming from an Event Hub.\nLet\u0026rsquo;s have a look at the details:\nWithin the dlt.table-decorator, you define the name of the table in the Hive-story, the path where the delta-table will be persisted and possibly partitioning columns. The format \u0026ldquo;cloudFiles\u0026rdquo; and the \u0026ldquo;cloudFiles.format\u0026rdquo;-option mark the Autoloader and define the input data format, here json. The \u0026ldquo;schemaHints\u0026rdquo;-option can be used to predefine the expected schema of the input data, using DDL Schema String. There are several options for schema evolution, i.e. how to handle cases where the schema changes over time. In the case of \u0026ldquo;rescue\u0026rdquo;, additional fields are collected in an additional \u0026ldquo;_rescue\u0026rdquo;-column. In the last row, we add a column that contains the file name. In our case, this was done because the filename contained additional data that was extracted in the following steps. ","date":"18 October 2022","permalink":"/blog/databricks-autoloader/","section":"Blog","summary":"Recently, we had the following challenge:","title":"Autoloader in Databricks Delta Live Tables"},{"content":"","date":null,"permalink":"/tags/databricks/","section":"Tags","summary":"","title":"Databricks"},{"content":"","date":null,"permalink":"/tags/apache-superset/","section":"Tags","summary":"","title":"Apache Superset"},{"content":"","date":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization"},{"content":"Apache Superset is a great open-source visualization and dashboarding tool that can be connected to a multitude of data sources. For many of those data sources, additional drivers/packages need to be installed.\nBut how to do that when using the default Docker-image? Also: how can we easily change superset-configurations. This is possible with a very simple custom Docker image.\nCreate a custom \u0026ldquo;Dockerfile\u0026rdquo;:\nFROM apache/superset:2.0.0 USER root RUN pip install duckdb-engine COPY ./start.sh /start.sh COPY superset_config.py /app/ ENV SUPERSET_CONFIG_PATH /app/superset_config.py USER superset ENTRYPOINT [ \u0026#34;/start.sh\u0026#34; ] This Dockerfile builds on top of the default Superset-image. Using the root-account, we install additional packages, here duckdb-engine because I want to connect to DuckDB (see here for other connections).\nThen we copy the start-script (see 2.) and the superset-config (see 3.) Finally, we switch back to the non-root user, as is best practice, and set the start-script as entrypoint.\nCreate a start-script start.sh. It is important that you make this file executable with chmod +x start.sh.\n#!/bin/bash superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password \u0026#34;admin\u0026#34; superset db upgrade superset init echo \u0026#34;Starting server\u0026#34; /bin/sh -c /usr/bin/run-server.sh This script creates an admin user \u0026ldquo;admin\u0026rdquo; with password \u0026ldquo;admin\u0026rdquo; (change accordingly and add other users with superset fab create-user). Then the database is upgraded and initialized. Finally the server is started.\nThis script can also be used to create roles at start-up.\nCreate a superset-config-file superset_config.py. Find here the original config-file from where you can get an idea of which configs can be set and modified.\nFEATURE_FLAGS = { \u0026#34;ENABLE_TEMPLATE_PROCESSING\u0026#34;: True, } def square(x): return x**2 JINJA_CONTEXT_ADDONS = { \u0026#39;my_crazy_macro\u0026#39;: square } This config is just an example. As you can see, this file can also be used to define functions that can be used as jinja-templates in the SQL-editor.\nAlso, we can define custom auth here (article to follow).\nBuild the Docker-image with docker build -f Dockerfile -t superset . and run with docker run -p 8088:8088 superset.\n","date":"8 October 2022","permalink":"/blog/superset-custom-image/","section":"Blog","summary":"Apache Superset is a great open-source visualization and dashboarding tool that can be connected to a multitude of data sources.","title":"Superset Custom Docker Image"},{"content":"","date":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"Azure Blob Storage - and especially the Datalake Gen 2 which is built on top and allows additional features like hierarchical namespace - is a great place to store data in all kinds of format: a data lake.\nHere, I explain how to connect to this data lake with Apache Spark and read in parquet-files from the data lake.\nRetrieve the access key from the Azure portal. Go to the storage account you want to connect to, choose \u0026ldquo;Access keys\u0026rdquo; under \u0026ldquo;Security + networking\u0026rdquo; and copy one of the keys. When creating your Spark Session in your application or notebook, you need to add one package that will be automatically downloaded and included in the session: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .master(\u0026#34;local\u0026#34;) \\ .config(\u0026#39;spark.jars.packages\u0026#39;, \u0026#39;org.apache.hadoop:hadoop-azure:3.3.1\u0026#39;)\\ .getOrCreate() Hand over the key from (1) to the Spark config. Note that the key allows access to your data and is therefore a secret! You should handle it properly, for example with a local env-file that is not checked into version control, or better something like a key vault. spark.conf.set( \u0026#34;fs.azure.account.key.\u0026lt;storage account name\u0026gt;.dfs.core.windows.net\u0026#34;, \u0026#34;\u0026lt;storage account key\u0026gt;\u0026#34; ) You can now read in parquet files: df = spark.read.format(\u0026#34;parquet\u0026#34;)\\ .load(\u0026#34;abfss://\u0026lt;container name\u0026gt;@\u0026lt;storage account name\u0026gt;.dfs.core.windows.net/\u0026lt;file path\u0026gt;\u0026#34;) Note that the container name comes before the \u0026ldquo;@\u0026quot;-sign but does not appear within the file path. ","date":"2 October 2022","permalink":"/blog/spark-read-from-azure-datalake/","section":"Blog","summary":"Azure Blob Storage - and especially the Datalake Gen 2 which is built on top and allows additional features like hierarchical namespace - is a great place to store data in all kinds of format: a data lake.","title":"Read parquet files from Azure Data Lake with Apache Spark"},{"content":"Apache Superset is a great open-source visualization and dashboarding tool that can be connected to a multitude of data sources.\nIn many situations it is required to adapt the layout, for example to match the corporate design.\nYou can add your own color scheme in the superset-config. This will, however, only change the color scheme of the typescript-plugins.\nThere are three other files that have to be changed to adjust the colors in the remaining css-files:\nsuperset-frontend/src/assets/stylesheets/antd/index.less superset-frontend/src/assets/stylesheets/less/variables.less (superset-frontend/packages/superset-ui-core/src/style/index.tsx; this file will be overwritten by the data from the config-file) Unfortunately, unlike the superset-configs, this requires us to rebuild the superset-frontend (automatically done when building a new Docker-image).\n","date":"20 August 2022","permalink":"/blog/superset-color-theme/","section":"Blog","summary":"Apache Superset is a great open-source visualization and dashboarding tool that can be connected to a multitude of data sources.","title":"Superset Color Theme"},{"content":"While regular backups of data came a bit out of fashion in the modern cloud world, there might be situations where you have to generate a copy of your data from a storage account. In my case, the customer required me to enable infrastructure-encryption of storage accounts which forces a resource recreation. So the question was how to backup the data before and then migrate it back to the new resources.\nThe solution is a bit hacky but works quite well: Create a temporary backup storage account, copy your data there, recreate the actual storage account, and then copy the data back. Here, I will explain how to do that using azcopy.\nCreate a backup account. While the rest of my setup is handled with terraform as infrastructure as Code (IaC), I did this manually in the Azure UI, as it is just temporary. It makes sense to choose the same settings as for the storage account that you want to backup.\nDownload azcopy (https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) and unpack it.\nLogin to your Azure account in the command line. You can do that with ./azcopy login which will lead you to a so-called device-login (meaning you should open a link in the browser, enter a alphanumeric code and then proceed with the normal Azure login). You might also need to choose the correct subscription in case you have several: az account set --subscription \u0026lt;subscription_id\u0026gt;. Also in some cases it can be necessary to add the tenantId: ./azcopy login --tenant-id \u0026lt;tenantId (you can get your tenant-id for example on the overview page of your \u0026ldquo;Azure Active Directory\u0026rdquo; resource in the Azure portal).\nNow create a Shared access signature for accessing the data with azcopy. Copy the resulting \u0026ldquo;Blob service SAS URL\u0026rdquo;. You can now list the files inside the storage account by typing ./azcopy list \u0026lt;SAS URL\u0026gt; (no additional parentheses).\nTo copy all data from one storage account to another (empty) storage account, you first need to get the SAS for the target account as well (see 4.) and then copy with the command: ./azcopy cp \u0026lt;SAS URL origin\u0026gt; \u0026lt;SAS URL destination\u0026gt; --recursive.\nNote that you can also copy on a per-container basis. In this case, choose the container, go to \u0026ldquo;Shared access tokens\u0026rdquo;, generate a SAS and proceed as above. Note that the default permissions for the SAS in this case is \u0026ldquo;Read\u0026rdquo; and you might have to add more.\nIn my case, I would now recreate the original storage account and copy all data back the same way. This might be a niche use case, but copying data of storage accounts with azcopy is certainly a useful skill also in other situations.\n","date":"20 August 2022","permalink":"/blog/backup-azure-storage/","section":"Blog","summary":"While regular backups of data came a bit out of fashion in the modern cloud world, there might be situations where you have to generate a copy of your data from a storage account.","title":"Backup Azure Storage Accounts"},{"content":"Gerold Busch #Blabla bla this is about me.\n","date":null,"permalink":"/about/","section":"About","summary":"Gerold Busch #Blabla bla this is about me.","title":"About"},{"content":"Here are all my posts\n","date":null,"permalink":"/blog/","section":"Blog","summary":"Here are all my posts","title":"Blog"},{"content":"Welcome to my homepage.\nThe main purpose of this page is to give me a space to write down some things that I learned about Data Science, Python, Apache Spark, etc. during my work - kind of a public notebook.\nIf you learn something about me and my work, or I can help you solve your problems faster using my notes\u0026hellip; That would be even better.\n","date":null,"permalink":"/","section":"Gerold Busch","summary":"Welcome to my homepage.","title":"Gerold Busch"},{"content":"Defining a schema in Spark can sometimes be tedious and confusing. Also, there are some use cases (for example when writing tests, article TBD) where it is of advantage to define the schema in one string.\nSchema definitions as a string follow the idea of the Data Definition Language. DDL expressions are for example used to create tables in SQL databases:\nCREATE TABLE employees (id INTEGER not null, name VARCHAR(50) not null); Here the expression\nid INTEGER not null, name VARCHAR(50) not null defines the schema of the table.\nThis notation can also be used to define schemas in Spark. For example, instead of defining:\nfrom pyspark.sql import types as T schema = T.StructType( [ T.StructField(\u0026#34;id\u0026#34;, T.IntegerType(), False), T.StructField(\u0026#34;name\u0026#34;, T.StringType(), True), ] ) you can simply write\nschema = \u0026#34;id int not null, name string\u0026#34; To convert the string into an actual schema-object, you can use the following function: from pyspark.sql.types import _parse_datatype_string. However, when passing it to the spark.createDataFrame-method, this is not needed and you can directly pass the string.\nNote that the notation also works for more complicated cases. However, when using nested schemas, you need to add colons between the column name and the data type. For example the following nested schema:\nfrom pyspark.sql import types as T schema = T.StructType( [ T.StructField(\u0026#34;id\u0026#34;, T.StringType(), False), T.StructField(\u0026#34;person\u0026#34;, T.ArrayType( T.StructType([ T.StructField(\u0026#34;name\u0026#34;, T.StringType(), False), T.StructField(\u0026#34;age\u0026#34;, T.IntegerType(), True), ])), False), ] ) becomes:\nid: int not null, person: array\u0026lt;struct\u0026lt;name: string not null, age: int\u0026gt;\u0026gt; ","date":"25 July 2022","permalink":"/blog/spark-ddl-schema/","section":"Blog","summary":"Defining a schema in Spark can sometimes be tedious and confusing.","title":"Defining Spark Schemas with Strings in DDL"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]